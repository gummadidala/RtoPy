#!/usr/bin/env python3
"""
Monthly Report Package 2 - Python conversion of Monthly_Report_Package_2.R
Packages pre-computed data from Package 1 for FlexDashboard reporting

This script:
1. Loads all .pkl files generated by monthly_report_package_1.py
2. Organizes them into cor/sub/sig structures (corridor/subcorridor/signal)
3. Generates quarterly aggregations from monthly data
4. Adds corridor metadata (Description column)
5. Uploads packaged data to S3
6. Writes summary metrics to database
"""

import os
import sys
import yaml
import pickle
import boto3
import logging
import pandas as pd
import numpy as np
import traceback
import gc
from datetime import datetime
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('monthly_report_package_2.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_pkl_data(filename):
    """Load pickle file data from data_output directory"""
    try:
        file_path = Path('data_output') / filename

        if file_path.exists():
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
                logger.debug(f"Loaded {filename}: {len(data) if isinstance(data, pd.DataFrame) else 'dict'} records")
                return data
        else:
            logger.warning(f"File not found: {file_path}")
            return pd.DataFrame()
    except Exception as e:
        logger.error(f"Error loading {filename}: {e}")
        return pd.DataFrame()


def get_quarterly(monthly_data, metric_col, weight_col=None, operation="sum"):
    """
    Convert monthly data to quarterly data - matches R get_quarterly() function
    
    Args:
        monthly_data: DataFrame with Month column
        metric_col: Name of metric column to aggregate
        weight_col: Optional weight column for weighted averages
        operation: "sum", "mean", or "latest"
    
    Returns:
        DataFrame with quarterly aggregations
    """
    try:
        if monthly_data.empty or metric_col not in monthly_data.columns:
            return pd.DataFrame()
        
        # Create a copy to avoid modifying original
        df = monthly_data.copy()
        
        # Ensure Month column is datetime
        if 'Month' in df.columns:
            df['Month'] = pd.to_datetime(df['Month'])
            df['Quarter'] = df['Month'].dt.to_period('Q')
        else:
            logger.warning(f"No Month column in data for {metric_col}")
            return pd.DataFrame()
        
        # Determine grouping columns
        group_cols = ['Quarter']
        for col in ['Zone_Group', 'Corridor', 'Zone']:
            if col in df.columns:
                group_cols.insert(0, col)
        
        # Perform aggregation based on operation
        if operation == "sum":
            result = df.groupby(group_cols, as_index=False)[metric_col].sum()
            
        elif operation == "latest":
            # Get the latest month's value for each quarter
            df_sorted = df.sort_values('Month')
            result = df_sorted.groupby(group_cols, as_index=False).last()
            result = result[group_cols + [metric_col]]
            
        else:  # mean (default for most metrics)
            if weight_col and weight_col in df.columns:
                # Weighted average
                df['weighted'] = df[metric_col] * df[weight_col]
                grouped = df.groupby(group_cols, as_index=False).agg({
                    'weighted': 'sum',
                    weight_col: 'sum'
                })
                grouped[metric_col] = grouped['weighted'] / grouped[weight_col]
                result = grouped[group_cols + [metric_col]]
            else:
                # Simple average
                result = df.groupby(group_cols, as_index=False)[metric_col].mean()
        
        # Convert Quarter back to timestamp for consistency
        result['Quarter'] = result['Quarter'].dt.to_timestamp()
        
        return result
        
    except Exception as e:
        logger.error(f"Error in get_quarterly for {metric_col}: {e}")
        return pd.DataFrame()


def sigify(signal_data, corridor_data, config_data, identifier='SignalID'):
    """
    Add corridor/zone information to signal-level data - matches R sigify() function
    
    Args:
        signal_data: DataFrame with signal-level data
        corridor_data: Reference corridor data (not used, kept for R compatibility)
        config_data: Configuration DataFrame with SignalID, Zone_Group, Corridor, Description
        identifier: Column name to join on (default 'SignalID', or 'CameraID' for CCTV)
    
    Returns:
        DataFrame with corridor information added
    """
    try:
        if signal_data.empty:
            return pd.DataFrame()
        
        if identifier not in signal_data.columns:
            logger.warning(f"Identifier '{identifier}' not in signal data")
            return signal_data
        
        # Select relevant columns from config
        config_cols = [identifier, 'Zone_Group', 'Corridor', 'Description']
        config_cols = [c for c in config_cols if c in config_data.columns]
        
        if identifier not in config_data.columns:
            logger.warning(f"Identifier '{identifier}' not in config data")
            return signal_data
        
        # Merge with config to add corridor info
        result = signal_data.merge(
            config_data[config_cols],
                on=identifier, 
            how='left',
            suffixes=('', '_config')
            )
        
        return result
            
    except Exception as e:
        logger.error(f"Error in sigify: {e}")
        return signal_data


def add_description_column(data_dict, config_data):
    """
    Add Description column to all dataframes in nested dictionary
    Matches R behavior: for (per in c("mo", "wk")) { ... mutate(Description = Corridor) }
    
    Args:
        data_dict: Nested dictionary with time periods and metrics
        config_data: Configuration data with Corridor to Description mapping
    """
    try:
        for period in ['mo', 'wk']:
            if period not in data_dict:
                continue
                
            for metric_name, metric_data in data_dict[period].items():
                if isinstance(metric_data, pd.DataFrame) and not metric_data.empty:
                    if 'Corridor' in metric_data.columns:
                        # Simple approach: Description = Corridor (as in R code)
                        metric_data['Description'] = metric_data['Corridor']
                        
    except Exception as e:
        logger.error(f"Error adding description column: {e}")


def get_corridor_summary_data(cor):
    """Generate corridor summary statistics - matches R function"""
    try:
        summary_data = {}
        
        # Average VPD from monthly data
        if 'mo' in cor and 'vpd' in cor['mo']:
            vpd_data = cor['mo']['vpd']
            if not vpd_data.empty and 'vpd' in vpd_data.columns:
                summary_data['avg_vpd'] = vpd_data.groupby('Corridor')['vpd'].mean().to_dict()
        
        # Average uptime from monthly data
        if 'mo' in cor and 'du' in cor['mo']:
            uptime_data = cor['mo']['du']
            if not uptime_data.empty and 'uptime' in uptime_data.columns:
                summary_data['avg_uptime'] = uptime_data.groupby('Corridor')['uptime'].mean().to_dict()
        
        return summary_data
        
    except Exception as e:
        logger.error(f"Error getting corridor summary data: {e}")
        return {}


def main():
    """Main processing function - equivalent to Monthly_Report_Package_2.R"""
    
    print(f"{datetime.now()} Package for Monthly Report [27 of 29 (mark1)]")
    
    try:
        # Load configuration
        with open('Monthly_Report.yaml', 'r') as f:
            conf = yaml.safe_load(f)
        
        # Load corridor and camera configurations
        corridors = load_pkl_data("corridors.pkl")
        subcorridors = load_pkl_data("subcorridors.pkl")
        cam_config = load_pkl_data("cam_config.pkl")
        
        if corridors.empty:
            logger.warning("No corridors configuration found - some features may not work")
        
        # ======================================================================
        # BUILD CORRIDOR (COR) DATA STRUCTURE
        # ======================================================================
        logger.info("Building corridor (cor) data structure...")
        
        cor = {}
        
        # === COR DAILY DATA ===
        cor['dy'] = {
            'du': load_pkl_data("cor_avg_daily_detector_uptime.pkl"),
            'cu': load_pkl_data("cor_daily_comm_uptime.pkl"),
            'pau': load_pkl_data("cor_daily_pa_uptime.pkl"),
            'cctv': load_pkl_data("cor_daily_cctv_uptime.pkl"),
        }
        
        # Task data - daily
        tasks_all = load_pkl_data("tasks_all.pkl")
        if isinstance(tasks_all, dict) and 'cor_daily' in tasks_all:
            tasks_daily = tasks_all['cor_daily']
            
            # Load tasks by type/subtype/priority/source
            tasks_by_type = load_pkl_data("tasks_by_type.pkl")
            tasks_by_subtype = load_pkl_data("tasks_by_subtype.pkl")
            tasks_by_priority = load_pkl_data("tasks_by_priority.pkl")
            tasks_by_source = load_pkl_data("tasks_by_source.pkl")
            
            cor['dy']['ttyp'] = tasks_by_type.get('cor_daily', pd.DataFrame()) if isinstance(tasks_by_type, dict) else pd.DataFrame()
            cor['dy']['tsub'] = tasks_by_subtype.get('cor_daily', pd.DataFrame()) if isinstance(tasks_by_subtype, dict) else pd.DataFrame()
            cor['dy']['tpri'] = tasks_by_priority.get('cor_daily', pd.DataFrame()) if isinstance(tasks_by_priority, dict) else pd.DataFrame()
            cor['dy']['tsou'] = tasks_by_source.get('cor_daily', pd.DataFrame()) if isinstance(tasks_by_source, dict) else pd.DataFrame()
            cor['dy']['tasks'] = tasks_daily
            
            # Extract specific metrics from tasks
            if not tasks_daily.empty:
                cor['dy']['reported'] = tasks_daily[['Zone_Group', 'Corridor', 'Date', 'Reported']].copy()
                cor['dy']['reported']['delta'] = np.nan
                
                cor['dy']['resolved'] = tasks_daily[['Zone_Group', 'Corridor', 'Date', 'Resolved']].copy()
                cor['dy']['resolved']['delta'] = np.nan
                
                cor['dy']['outstanding'] = tasks_daily[['Zone_Group', 'Corridor', 'Date', 'Outstanding']].copy()
                cor['dy']['outstanding']['delta'] = np.nan
        
        # === COR WEEKLY DATA ===
        cor['wk'] = {
            'vpd': load_pkl_data("cor_weekly_vpd.pkl"),
            'papd': load_pkl_data("cor_weekly_papd.pkl"),
            'pd': load_pkl_data("cor_weekly_pd_by_day.pkl"),
            'tp': load_pkl_data("cor_weekly_throughput.pkl"),
            'aogd': load_pkl_data("cor_weekly_aog_by_day.pkl"),
            'prd': load_pkl_data("cor_weekly_pr_by_day.pkl"),
            'qsd': load_pkl_data("cor_wqs.pkl"),
            'sfd': load_pkl_data("cor_wsf.pkl"),
            'sfo': load_pkl_data("cor_wsfo.pkl"),
            'du': load_pkl_data("cor_weekly_detector_uptime.pkl"),
            'cu': load_pkl_data("cor_weekly_comm_uptime.pkl"),
            'pau': load_pkl_data("cor_weekly_pa_uptime.pkl"),
            'cctv': load_pkl_data("cor_weekly_cctv_uptime.pkl"),
        }
        
        # Weekly peak hour data
        weekly_vph_peak = load_pkl_data("cor_weekly_vph_peak.pkl")
        if isinstance(weekly_vph_peak, dict):
            cor['wk']['vphpa'] = weekly_vph_peak.get('am', pd.DataFrame())
            cor['wk']['vphpp'] = weekly_vph_peak.get('pm', pd.DataFrame())
        else:
            cor['wk']['vphpa'] = pd.DataFrame()
            cor['wk']['vphpp'] = pd.DataFrame()
        
        # === COR MONTHLY DATA ===
        cor['mo'] = {
            'vpd': load_pkl_data("cor_monthly_vpd.pkl"),
            'papd': load_pkl_data("cor_monthly_papd.pkl"),
            'pd': load_pkl_data("cor_monthly_pd_by_day.pkl"),
            'tp': load_pkl_data("cor_monthly_throughput.pkl"),
            'aogd': load_pkl_data("cor_monthly_aog_by_day.pkl"),
            'aogh': load_pkl_data("cor_monthly_aog_by_hr.pkl"),
            'prd': load_pkl_data("cor_monthly_pr_by_day.pkl"),
            'prh': load_pkl_data("cor_monthly_pr_by_hr.pkl"),
            'qsd': load_pkl_data("cor_monthly_qsd.pkl"),
            'qsh': load_pkl_data("cor_mqsh.pkl"),
            'sfd': load_pkl_data("cor_monthly_sfd.pkl"),
            'sfh': load_pkl_data("cor_msfh.pkl"),
            'sfo': load_pkl_data("cor_monthly_sfo.pkl"),
            'tti': load_pkl_data("cor_monthly_tti.pkl"),
            'ttih': load_pkl_data("cor_monthly_tti_by_hr.pkl"),
            'pti': load_pkl_data("cor_monthly_pti.pkl"),
            'ptih': load_pkl_data("cor_monthly_pti_by_hr.pkl"),
            'bi': load_pkl_data("cor_monthly_bi.pkl"),
            'bih': load_pkl_data("cor_monthly_bi_by_hr.pkl"),
            'spd': load_pkl_data("cor_monthly_spd.pkl"),
            'spdh': load_pkl_data("cor_monthly_spd_by_hr.pkl"),
            'du': load_pkl_data("cor_monthly_detector_uptime.pkl"),
            'cu': load_pkl_data("cor_monthly_comm_uptime.pkl"),
            'pau': load_pkl_data("cor_monthly_pa_uptime.pkl"),
            'cctv': load_pkl_data("cor_monthly_cctv_uptime.pkl"),
            'flash': load_pkl_data("cor_monthly_flash.pkl"),
            'bpsi': load_pkl_data("cor_monthly_bpsi.pkl"),
            'rsi': load_pkl_data("cor_monthly_rsi.pkl"),
            'cri': load_pkl_data("cor_monthly_crash_rate_index.pkl"),
            'kabco': load_pkl_data("cor_monthly_kabco_index.pkl"),
        }
        
        # Monthly peak hour data
        monthly_vph_peak = load_pkl_data("cor_monthly_vph_peak.pkl")
        if isinstance(monthly_vph_peak, dict):
            cor['mo']['vphpa'] = monthly_vph_peak.get('am', pd.DataFrame())
            cor['mo']['vphpp'] = monthly_vph_peak.get('pm', pd.DataFrame())
        else:
            cor['mo']['vphpa'] = pd.DataFrame()
            cor['mo']['vphpp'] = pd.DataFrame()
        
        # Monthly task data
        if isinstance(tasks_all, dict) and 'cor_monthly' in tasks_all:
            tasks_monthly = tasks_all['cor_monthly']
            
            tasks_by_type = load_pkl_data("tasks_by_type.pkl")
            tasks_by_subtype = load_pkl_data("tasks_by_subtype.pkl")
            tasks_by_priority = load_pkl_data("tasks_by_priority.pkl")
            tasks_by_source = load_pkl_data("tasks_by_source.pkl")
            
            cor['mo']['ttyp'] = tasks_by_type.get('cor_monthly', pd.DataFrame()) if isinstance(tasks_by_type, dict) else pd.DataFrame()
            cor['mo']['tsub'] = tasks_by_subtype.get('cor_monthly', pd.DataFrame()) if isinstance(tasks_by_subtype, dict) else pd.DataFrame()
            cor['mo']['tpri'] = tasks_by_priority.get('cor_monthly', pd.DataFrame()) if isinstance(tasks_by_priority, dict) else pd.DataFrame()
            cor['mo']['tsou'] = tasks_by_source.get('cor_monthly', pd.DataFrame()) if isinstance(tasks_by_source, dict) else pd.DataFrame()
            cor['mo']['tasks'] = tasks_monthly
            
            # Extract specific metrics with deltas
            if not tasks_monthly.empty:
                cor['mo']['reported'] = tasks_monthly[['Zone_Group', 'Corridor', 'Month', 'Reported']].copy()
                if 'delta.rep' in tasks_monthly.columns:
                    cor['mo']['reported']['delta'] = tasks_monthly['delta.rep']
                else:
                    cor['mo']['reported']['delta'] = np.nan
                
                cor['mo']['resolved'] = tasks_monthly[['Zone_Group', 'Corridor', 'Month', 'Resolved']].copy()
                if 'delta.res' in tasks_monthly.columns:
                    cor['mo']['resolved']['delta'] = tasks_monthly['delta.res']
                else:
                    cor['mo']['resolved']['delta'] = np.nan
                
                cor['mo']['outstanding'] = tasks_monthly[['Zone_Group', 'Corridor', 'Month', 'Outstanding']].copy()
                if 'delta.out' in tasks_monthly.columns:
                    cor['mo']['outstanding']['delta'] = tasks_monthly['delta.out']
                else:
                    cor['mo']['outstanding']['delta'] = np.nan
        
        # Task performance metrics
        tasks_by_date = load_pkl_data("cor_tasks_by_date.pkl")
        if not tasks_by_date.empty:
            cor['mo']['over45'] = tasks_by_date[['Zone_Group', 'Corridor', 'Month', 'over45']].copy()
            if 'delta.over45' in tasks_by_date.columns:
                cor['mo']['over45']['delta'] = tasks_by_date['delta.over45']
            else:
                cor['mo']['over45']['delta'] = np.nan
                
            cor['mo']['mttr'] = tasks_by_date[['Zone_Group', 'Corridor', 'Month', 'mttr']].copy()
            if 'delta.mttr' in tasks_by_date.columns:
                cor['mo']['mttr']['delta'] = tasks_by_date['delta.mttr']
            else:
                cor['mo']['mttr']['delta'] = np.nan
        
        # User delay costs
        hourly_udc = load_pkl_data("hourly_udc.pkl")
        if not hourly_udc.empty:
            cor['mo']['hourly_udc'] = hourly_udc
            
            # Load UDC trend table if available
            udc_trend = load_pkl_data("udc_trend_table_list.pkl")
            if isinstance(udc_trend, (dict, list)):
                cor['mo']['udc_trend_table'] = udc_trend
        
        # Hourly corridor data
        cor['mo']['vphh'] = load_pkl_data("cor_monthly_vph.pkl")
        cor['mo']['paph'] = load_pkl_data("cor_monthly_paph.pkl")
        
        # === COR QUARTERLY DATA ===
        logger.info("Generating quarterly corridor data...")
        
        cor['qu'] = {
            'vpd': get_quarterly(cor['mo']['vpd'], 'vpd', operation="sum"),
            'vphpa': get_quarterly(cor['mo']['vphpa'], 'vph', operation="sum"),
            'vphpp': get_quarterly(cor['mo']['vphpp'], 'vph', operation="sum"),
            'papd': get_quarterly(cor['mo']['papd'], 'papd', operation="sum"),
            'pd': get_quarterly(cor['mo']['pd'], 'pd', weight_col=None, operation="mean"),
            'tp': get_quarterly(cor['mo']['tp'], 'vph', weight_col=None, operation="mean"),
            'aogd': get_quarterly(cor['mo']['aogd'], 'aog', weight_col='vol', operation="mean"),
            'prd': get_quarterly(cor['mo']['prd'], 'pr', weight_col='vol', operation="mean"),
            'qsd': get_quarterly(cor['mo']['qsd'], 'qs_freq', operation="mean"),
            'sfd': get_quarterly(cor['mo']['sfd'], 'sf_freq', operation="mean"),
            'sfo': get_quarterly(cor['mo']['sfo'], 'sf_freq', operation="mean"),
            'tti': get_quarterly(cor['mo']['tti'], 'tti', operation="mean"),
            'pti': get_quarterly(cor['mo']['pti'], 'pti', operation="mean"),
            'bi': get_quarterly(cor['mo']['bi'], 'bi', operation="mean"),
            'spd': get_quarterly(cor['mo']['spd'], 'speed_mph', operation="mean"),
            'du': get_quarterly(cor['mo']['du'], 'uptime', operation="mean"),
            'cu': get_quarterly(cor['mo']['cu'], 'uptime', operation="mean"),
            'pau': get_quarterly(cor['mo']['pau'], 'uptime', operation="mean"),
            'cctv': get_quarterly(cor['mo']['cctv'], 'uptime', weight_col='num', operation="mean"),
            'bpsi': get_quarterly(cor['mo']['bpsi'], 'bpsi', operation="latest"),
            'rsi': get_quarterly(cor['mo']['rsi'], 'rsi', operation="latest"),
            'cri': get_quarterly(cor['mo']['cri'], 'cri', operation="latest"),
            'kabco': get_quarterly(cor['mo']['kabco'], 'kabco', operation="latest"),
        }
        
        # Add quarterly task metrics if monthly tasks exist
        if 'tasks' in cor['mo'] and not cor['mo']['tasks'].empty:
            cor['qu']['reported'] = get_quarterly(cor['mo']['tasks'], 'Reported', operation="sum")
            cor['qu']['resolved'] = get_quarterly(cor['mo']['tasks'], 'Resolved', operation="sum")
            cor['qu']['outstanding'] = get_quarterly(cor['mo']['tasks'], 'Outstanding', operation="latest")
        
        if 'over45' in cor['mo'] and not cor['mo']['over45'].empty:
            cor['qu']['over45'] = get_quarterly(cor['mo']['over45'], 'over45', operation="latest")
        
        if 'mttr' in cor['mo'] and not cor['mo']['mttr'].empty:
            cor['qu']['mttr'] = get_quarterly(cor['mo']['mttr'], 'mttr', operation="latest")
        
        # Add Description column to corridor data (matches R: mutate(Description = Corridor))
        add_description_column(cor, corridors)
        
        # Add corridor summary data
        cor['summary_data'] = get_corridor_summary_data(cor)
        
        logger.info(f"Corridor data structure built: {len(cor)} time periods")
        
        # ======================================================================
        # BUILD SUBCORRIDOR (SUB) DATA STRUCTURE
        # ======================================================================
        logger.info("Building subcorridor (sub) data structure...")
        
        sub = {}
        
        # === SUB DAILY DATA ===
        sub['dy'] = {
            'du': load_pkl_data("sub_avg_daily_detector_uptime.pkl"),
            'cu': load_pkl_data("sub_daily_comm_uptime.pkl"),
            'pau': load_pkl_data("sub_daily_pa_uptime.pkl"),
            'cctv': load_pkl_data("sub_daily_cctv_uptime.pkl"),
        }
        
        # Select specific columns as in R code
        for key in sub['dy']:
            df = sub['dy'][key]
            if not df.empty:
                available_cols = [c for c in ['Zone_Group', 'Corridor', 'Date', 'uptime', 'uptime.sb', 'uptime.pr'] if c in df.columns]
                if available_cols:
                    sub['dy'][key] = df[available_cols]
        
        # === SUB WEEKLY DATA ===
        sub['wk'] = {
            'vpd': load_pkl_data("sub_weekly_vpd.pkl"),
            'papd': load_pkl_data("sub_weekly_papd.pkl"),
            'pd': load_pkl_data("sub_weekly_pd_by_day.pkl"),
            'tp': load_pkl_data("sub_weekly_throughput.pkl"),
            'aogd': load_pkl_data("sub_weekly_aog_by_day.pkl"),
            'prd': load_pkl_data("sub_weekly_pr_by_day.pkl"),
            'qsd': load_pkl_data("sub_wqs.pkl"),
            'sfd': load_pkl_data("sub_wsf.pkl"),
            'sfo': load_pkl_data("sub_wsfo.pkl"),
            'du': load_pkl_data("sub_weekly_detector_uptime.pkl"),
            'cu': load_pkl_data("sub_weekly_comm_uptime.pkl"),
            'pau': load_pkl_data("sub_weekly_pa_uptime.pkl"),
            'cctv': load_pkl_data("sub_weekly_cctv_uptime.pkl"),
        }
        
        # Weekly peak hour data
        sub_weekly_vph_peak = load_pkl_data("sub_weekly_vph_peak.pkl")
        if isinstance(sub_weekly_vph_peak, dict):
            sub['wk']['vphpa'] = sub_weekly_vph_peak.get('am', pd.DataFrame())
            sub['wk']['vphpp'] = sub_weekly_vph_peak.get('pm', pd.DataFrame())
        else:
            sub['wk']['vphpa'] = pd.DataFrame()
            sub['wk']['vphpp'] = pd.DataFrame()
        
        # Select specific columns for weekly data as in R
        for key, metric_col in [('vpd', 'vpd'), ('vphpa', 'vph'), ('vphpp', 'vph'), 
                                 ('papd', 'papd'), ('pd', 'pd'), ('tp', 'vph'),
                                 ('aogd', 'aog'), ('prd', 'pr'), ('qsd', 'qs_freq'),
                                 ('sfd', 'sf_freq'), ('sfo', 'sf_freq'), 
                                 ('du', 'uptime'), ('cu', 'uptime'), ('pau', 'uptime'), ('cctv', 'uptime')]:
            df = sub['wk'][key]
            if not df.empty:
                available_cols = [c for c in ['Zone_Group', 'Corridor', 'Date', metric_col] if c in df.columns]
                if available_cols:
                    sub['wk'][key] = df[available_cols]
        
        # === SUB MONTHLY DATA ===
        sub['mo'] = {
            'vpd': load_pkl_data("sub_monthly_vpd.pkl"),
            'papd': load_pkl_data("sub_monthly_papd.pkl"),
            'pd': load_pkl_data("sub_monthly_pd_by_day.pkl"),
            'tp': load_pkl_data("sub_monthly_throughput.pkl"),
            'aogd': load_pkl_data("sub_monthly_aog_by_day.pkl"),
            'aogh': load_pkl_data("sub_monthly_aog_by_hr.pkl"),
            'prd': load_pkl_data("sub_monthly_pr_by_day.pkl"),
            'prh': load_pkl_data("sub_monthly_pr_by_hr.pkl"),
            'qsd': load_pkl_data("sub_monthly_qsd.pkl"),
            'qsh': load_pkl_data("sub_mqsh.pkl"),
            'sfd': load_pkl_data("sub_monthly_sfd.pkl"),
            'sfo': load_pkl_data("sub_monthly_sfo.pkl"),
            'sfh': load_pkl_data("sub_msfh.pkl"),
            'tti': load_pkl_data("sub_monthly_tti.pkl"),
            'ttih': load_pkl_data("sub_monthly_tti_by_hr.pkl"),
            'pti': load_pkl_data("sub_monthly_pti.pkl"),
            'ptih': load_pkl_data("sub_monthly_pti_by_hr.pkl"),
            'bi': load_pkl_data("sub_monthly_bi.pkl"),
            'bih': load_pkl_data("sub_monthly_bi_by_hr.pkl"),
            'spd': load_pkl_data("sub_monthly_spd.pkl"),
            'spdh': load_pkl_data("sub_monthly_spd_by_hr.pkl"),
            'du': load_pkl_data("sub_monthly_detector_uptime.pkl"),
            'cu': load_pkl_data("sub_monthly_comm_uptime.pkl"),
            'pau': load_pkl_data("sub_monthly_pa_uptime.pkl"),
            'cctv': load_pkl_data("sub_monthly_cctv_uptime.pkl"),
            'flash': load_pkl_data("sub_monthly_flash.pkl"),
            'bpsi': load_pkl_data("sub_monthly_bpsi.pkl"),
            'rsi': load_pkl_data("sub_monthly_rsi.pkl"),
            'cri': load_pkl_data("sub_monthly_crash_rate_index.pkl"),
            'kabco': load_pkl_data("sub_monthly_kabco_index.pkl"),
        }
        
        # Monthly peak hour data
        sub_monthly_vph_peak = load_pkl_data("sub_monthly_vph_peak.pkl")
        if isinstance(sub_monthly_vph_peak, dict):
            sub['mo']['vphpa'] = sub_monthly_vph_peak.get('am', pd.DataFrame())
            sub['mo']['vphpp'] = sub_monthly_vph_peak.get('pm', pd.DataFrame())
        else:
            sub['mo']['vphpa'] = pd.DataFrame()
            sub['mo']['vphpp'] = pd.DataFrame()
        
        # Hourly subcorridor data
        sub['mo']['vphh'] = load_pkl_data("sub_monthly_vph.pkl")
        sub['mo']['paph'] = load_pkl_data("sub_monthly_paph.pkl")
        
        # === SUB QUARTERLY DATA ===
        logger.info("Generating quarterly subcorridor data...")
        
        sub['qu'] = {
            'vpd': get_quarterly(sub['mo']['vpd'], 'vpd', operation="sum"),
            'vphpa': get_quarterly(sub['mo']['vphpa'], 'vph', operation="sum"),
            'vphpp': get_quarterly(sub['mo']['vphpp'], 'vph', operation="sum"),
            'papd': get_quarterly(sub['mo']['papd'], 'papd', operation="sum"),
            'pd': get_quarterly(sub['mo']['pd'], 'pd', operation="mean"),
            'tp': get_quarterly(sub['mo']['tp'], 'vph', operation="mean"),
            'aogd': get_quarterly(sub['mo']['aogd'], 'aog', weight_col='vol', operation="mean"),
            'prd': get_quarterly(sub['mo']['prd'], 'pr', weight_col='vol', operation="mean"),
            'qsd': get_quarterly(sub['mo']['qsd'], 'qs_freq', operation="mean"),
            'sfd': get_quarterly(sub['mo']['sfd'], 'sf_freq', operation="mean"),
            'sfo': get_quarterly(sub['mo']['sfo'], 'sf_freq', operation="mean"),
            'tti': get_quarterly(sub['mo']['tti'], 'tti', operation="mean"),
            'pti': get_quarterly(sub['mo']['pti'], 'pti', operation="mean"),
            'bi': get_quarterly(sub['mo']['bi'], 'bi', operation="mean"),
            'spd': get_quarterly(sub['mo']['spd'], 'speed_mph', operation="mean"),
            'du': get_quarterly(sub['mo']['du'], 'uptime', operation="mean"),
            'cu': get_quarterly(sub['mo']['cu'], 'uptime', operation="mean"),
            'pau': get_quarterly(sub['mo']['pau'], 'uptime', operation="mean"),
            'cctv': get_quarterly(sub['mo']['cctv'], 'uptime', operation="mean"),
        }
        
        # Add Description column to subcorridor data
        add_description_column(sub, subcorridors if not subcorridors.empty else corridors)
        
        logger.info(f"Subcorridor data structure built: {len(sub)} time periods")
        
        # ======================================================================
        # BUILD SIGNAL (SIG) DATA STRUCTURE
        # ======================================================================
        logger.info("Building signal (sig) data structure...")
        
        sig = {}
        
        # === SIG DAILY DATA ===
        sig['dy'] = {}
        
        if not corridors.empty:
            # Apply sigify to add corridor information to signal-level data
            du_daily = load_pkl_data("avg_daily_detector_uptime.pkl")
            if not du_daily.empty:
                sig['dy']['du'] = sigify(du_daily, cor['dy'].get('du', pd.DataFrame()), corridors)
                # Select specific columns
                cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime', 'uptime.sb', 'uptime.pr'] if c in sig['dy']['du'].columns]
                if cols:
                    sig['dy']['du'] = sig['dy']['du'][cols]
            
            cu_daily = load_pkl_data("daily_comm_uptime.pkl")
            if not cu_daily.empty:
                sig['dy']['cu'] = sigify(cu_daily, cor['dy'].get('cu', pd.DataFrame()), corridors)
                cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime'] if c in sig['dy']['cu'].columns]
                if cols:
                    sig['dy']['cu'] = sig['dy']['cu'][cols]
            
            pau_daily = load_pkl_data("daily_pa_uptime.pkl")
            if not pau_daily.empty:
                sig['dy']['pau'] = sigify(pau_daily, cor['dy'].get('pau', pd.DataFrame()), corridors)
                cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime'] if c in sig['dy']['pau'].columns]
                if cols:
                    sig['dy']['pau'] = sig['dy']['pau'][cols]
            
            # CCTV uses cam_config with CameraID
            if not cam_config.empty:
                cctv_daily = load_pkl_data("daily_cctv_uptime.pkl")
                if not cctv_daily.empty:
                    sig['dy']['cctv'] = sigify(cctv_daily, cor['dy'].get('cctv', pd.DataFrame()), cam_config, identifier='CameraID')
                    cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime', 'up'] if c in sig['dy']['cctv'].columns]
                    if cols:
                        sig['dy']['cctv'] = sig['dy']['cctv'][cols]
        
        # === SIG WEEKLY DATA ===
        sig['wk'] = {}
        
        if not corridors.empty:
            # Apply sigify for weekly signal data
            weekly_data = {
                'vpd': ('weekly_vpd.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'vpd']),
                'papd': ('weekly_papd.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'papd']),
                'pd': ('weekly_pd_by_day.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'pd']),
                'tp': ('weekly_throughput.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'vph']),
                'aogd': ('weekly_aog_by_day.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'aog']),
                'prd': ('weekly_pr_by_day.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'pr']),
                'qsd': ('wqs.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'qs_freq']),
                'sfd': ('wsf.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'sf_freq']),
                'sfo': ('wsfo.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'sf_freq']),
                'du': ('weekly_detector_uptime.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime']),
                'cu': ('weekly_comm_uptime.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime']),
                'pau': ('weekly_pa_uptime.pkl', ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime']),
            }
            
            # Handle weekly peak VPH separately
            weekly_vph_peak_sig = load_pkl_data("weekly_vph_peak.pkl")
            if isinstance(weekly_vph_peak_sig, dict):
                if 'am' in weekly_vph_peak_sig and not weekly_vph_peak_sig['am'].empty:
                    sig['wk']['vphpa'] = sigify(weekly_vph_peak_sig['am'], cor['wk'].get('vphpa', pd.DataFrame()), corridors)
                    cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Date', 'vph'] if c in sig['wk']['vphpa'].columns]
                    if cols:
                        sig['wk']['vphpa'] = sig['wk']['vphpa'][cols]
                
                if 'pm' in weekly_vph_peak_sig and not weekly_vph_peak_sig['pm'].empty:
                    sig['wk']['vphpp'] = sigify(weekly_vph_peak_sig['pm'], cor['wk'].get('vphpp', pd.DataFrame()), corridors)
                    cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Date', 'vph'] if c in sig['wk']['vphpp'].columns]
                    if cols:
                        sig['wk']['vphpp'] = sig['wk']['vphpp'][cols]
            
            # Process other weekly data
            for key, (filename, select_cols) in weekly_data.items():
                data = load_pkl_data(filename)
                if not data.empty:
                    sig['wk'][key] = sigify(data, cor['wk'].get(key, pd.DataFrame()), corridors)
                    available_cols = [c for c in select_cols if c in sig['wk'][key].columns]
                    if available_cols:
                        sig['wk'][key] = sig['wk'][key][available_cols]
            
            # CCTV weekly with cam_config
            if not cam_config.empty:
                cctv_weekly = load_pkl_data("weekly_cctv_uptime.pkl")
                if not cctv_weekly.empty:
                    sig['wk']['cctv'] = sigify(cctv_weekly, cor['wk'].get('cctv', pd.DataFrame()), cam_config, identifier='CameraID')
                    cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Date', 'uptime'] if c in sig['wk']['cctv'].columns]
                    if cols:
                        sig['wk']['cctv'] = sig['wk']['cctv'][cols]
        
        # === SIG MONTHLY DATA ===
        sig['mo'] = {}
        
        if not corridors.empty:
            # Monthly signal data with sigify
            monthly_data = {
                'vpd': ('monthly_vpd.pkl', ['Name', 'ones']),
                'papd': ('monthly_papd.pkl', ['Name', 'ones']),
                'pd': ('monthly_pd_by_day.pkl', ['Name', 'Events']),
                'tp': ('monthly_throughput.pkl', ['Name', 'ones']),
                'aogd': ('monthly_aog_by_day.pkl', ['Name', 'vol']),
                'aogh': ('monthly_aog_by_hr.pkl', ['Name', 'vol']),
                'prd': ('monthly_pr_by_day.pkl', ['Name', 'vol']),
                'prh': ('monthly_pr_by_hr.pkl', ['Name', 'vol']),
                'qsd': ('monthly_qsd.pkl', ['Name', 'cycles']),
                'qsh': ('mqsh.pkl', ['Name', 'cycles']),
                'sfd': ('monthly_sfd.pkl', ['Name', 'cycles']),
                'sfh': ('msfh.pkl', ['Name', 'cycles']),
                'sfo': ('monthly_sfo.pkl', ['Name', 'cycles']),
                'flash': ('monthly_flash.pkl', ['Name', 'ones']),
            }
            
            # Handle monthly peak VPH separately
            monthly_vph_peak_sig = load_pkl_data("monthly_vph_peak.pkl")
            if isinstance(monthly_vph_peak_sig, dict):
                if 'am' in monthly_vph_peak_sig and not monthly_vph_peak_sig['am'].empty:
                    sig['mo']['vphpa'] = sigify(monthly_vph_peak_sig['am'], cor['mo'].get('vphpa', pd.DataFrame()), corridors)
                    # Drop unwanted columns
                    drop_cols = [c for c in ['Name', 'ones'] if c in sig['mo']['vphpa'].columns]
                    if drop_cols:
                        sig['mo']['vphpa'] = sig['mo']['vphpa'].drop(columns=drop_cols)
                
                if 'pm' in monthly_vph_peak_sig and not monthly_vph_peak_sig['pm'].empty:
                    sig['mo']['vphpp'] = sigify(monthly_vph_peak_sig['pm'], cor['mo'].get('vphpp', pd.DataFrame()), corridors)
                    drop_cols = [c for c in ['Name', 'ones'] if c in sig['mo']['vphpp'].columns]
                    if drop_cols:
                        sig['mo']['vphpp'] = sig['mo']['vphpp'].drop(columns=drop_cols)
            
            # Process other monthly data
            for key, (filename, drop_cols_list) in monthly_data.items():
                data = load_pkl_data(filename)
                if not data.empty:
                    sig['mo'][key] = sigify(data, cor['mo'].get(key, pd.DataFrame()), corridors)
                    # Drop unwanted columns
                    drop_cols = [c for c in drop_cols_list if c in sig['mo'][key].columns]
                    if drop_cols:
                        sig['mo'][key] = sig['mo'][key].drop(columns=drop_cols)
            
            # Travel time indices (empty in R code)
            sig['mo']['tti'] = pd.DataFrame()
            sig['mo']['pti'] = pd.DataFrame()
            sig['mo']['bi'] = pd.DataFrame()
            sig['mo']['spd'] = pd.DataFrame()
            
            # Uptime data with specific columns
            du_monthly = load_pkl_data("monthly_detector_uptime.pkl")
            if not du_monthly.empty:
                sig['mo']['du'] = sigify(du_monthly, cor['mo'].get('du', pd.DataFrame()), corridors)
                cols = [c for c in ['Zone_Group', 'Corridor', 'Month', 'uptime', 'uptime.sb', 'uptime.pr', 'delta'] if c in sig['mo']['du'].columns]
                if cols:
                    sig['mo']['du'] = sig['mo']['du'][cols]
            
            cu_monthly = load_pkl_data("monthly_comm_uptime.pkl")
            if not cu_monthly.empty:
                sig['mo']['cu'] = sigify(cu_monthly, cor['mo'].get('cu', pd.DataFrame()), corridors)
                cols = [c for c in ['Zone_Group', 'Corridor', 'Month', 'uptime', 'delta'] if c in sig['mo']['cu'].columns]
                if cols:
                    sig['mo']['cu'] = sig['mo']['cu'][cols]
            
            pau_monthly = load_pkl_data("monthly_pa_uptime.pkl")
            if not pau_monthly.empty:
                sig['mo']['pau'] = sigify(pau_monthly, cor['mo'].get('pau', pd.DataFrame()), corridors)
                cols = [c for c in ['Zone_Group', 'Corridor', 'Month', 'uptime', 'delta'] if c in sig['mo']['pau'].columns]
                if cols:
                    sig['mo']['pau'] = sig['mo']['pau'][cols]
            
            # CCTV monthly with cam_config
            if not cam_config.empty:
                cctv_monthly = load_pkl_data("monthly_cctv_uptime.pkl")
                if not cctv_monthly.empty:
                    sig['mo']['cctv'] = sigify(cctv_monthly, cor['mo'].get('cctv', pd.DataFrame()), cam_config, identifier='CameraID')
                    cols = [c for c in ['Zone_Group', 'Corridor', 'Description', 'Month', 'uptime', 'delta'] if c in sig['mo']['cctv'].columns]
                    if cols:
                        sig['mo']['cctv'] = sig['mo']['cctv'][cols]
            
            # Crash indices
            cri_monthly = load_pkl_data("monthly_crash_rate_index.pkl")
            if not cri_monthly.empty:
                sig['mo']['cri'] = sigify(cri_monthly, cor['mo'].get('cri', pd.DataFrame()), corridors)
                cols = [c for c in ['Zone_Group', 'Corridor', 'Month', 'cri', 'delta'] if c in sig['mo']['cri'].columns]
                if cols:
                    sig['mo']['cri'] = sig['mo']['cri'][cols]
            
            kabco_monthly = load_pkl_data("monthly_kabco_index.pkl")
            if not kabco_monthly.empty:
                sig['mo']['kabco'] = sigify(kabco_monthly, cor['mo'].get('kabco', pd.DataFrame()), corridors)
                cols = [c for c in ['Zone_Group', 'Corridor', 'Month', 'kabco', 'delta'] if c in sig['mo']['kabco'].columns]
                if cols:
                    sig['mo']['kabco'] = sig['mo']['kabco'][cols]
        
        # Signal monthly task data
        if isinstance(tasks_all, dict) and 'sig_monthly' in tasks_all:
            tasks_by_type = load_pkl_data("tasks_by_type.pkl")
            tasks_by_subtype = load_pkl_data("tasks_by_subtype.pkl")
            tasks_by_priority = load_pkl_data("tasks_by_priority.pkl")
            tasks_by_source = load_pkl_data("tasks_by_source.pkl")
            
            sig['mo']['ttyp'] = tasks_by_type.get('sig_monthly', pd.DataFrame()) if isinstance(tasks_by_type, dict) else pd.DataFrame()
            sig['mo']['tsub'] = tasks_by_subtype.get('sig_monthly', pd.DataFrame()) if isinstance(tasks_by_subtype, dict) else pd.DataFrame()
            sig['mo']['tpri'] = tasks_by_priority.get('sig_monthly', pd.DataFrame()) if isinstance(tasks_by_priority, dict) else pd.DataFrame()
            sig['mo']['tsou'] = tasks_by_source.get('sig_monthly', pd.DataFrame()) if isinstance(tasks_by_source, dict) else pd.DataFrame()
            sig['mo']['tasks'] = tasks_all['sig_monthly']
            
            tasks_sig_monthly = tasks_all['sig_monthly']
            if not tasks_sig_monthly.empty:
                sig['mo']['reported'] = tasks_sig_monthly[['Zone_Group', 'Corridor', 'Month', 'Reported']].copy()
                if 'delta.rep' in tasks_sig_monthly.columns:
                    sig['mo']['reported']['delta'] = tasks_sig_monthly['delta.rep']
                else:
                    sig['mo']['reported']['delta'] = np.nan
                
                sig['mo']['resolved'] = tasks_sig_monthly[['Zone_Group', 'Corridor', 'Month', 'Resolved']].copy()
                if 'delta.res' in tasks_sig_monthly.columns:
                    sig['mo']['resolved']['delta'] = tasks_sig_monthly['delta.res']
            else:
                sig['mo']['resolved']['delta'] = np.nan
                
                sig['mo']['outstanding'] = tasks_sig_monthly[['Zone_Group', 'Corridor', 'Month', 'Outstanding']].copy()
                if 'delta.out' in tasks_sig_monthly.columns:
                    sig['mo']['outstanding']['delta'] = tasks_sig_monthly['delta.out']
                else:
                    sig['mo']['outstanding']['delta'] = np.nan
        
        # Signal task performance metrics
        sig_tasks_by_date = load_pkl_data("sig_tasks_by_date.pkl")
        if not sig_tasks_by_date.empty:
            sig['mo']['over45'] = sig_tasks_by_date[['Zone_Group', 'Corridor', 'Month', 'over45']].copy()
            if 'delta.over45' in sig_tasks_by_date.columns:
                sig['mo']['over45']['delta'] = sig_tasks_by_date['delta.over45']
            else:
                sig['mo']['over45']['delta'] = np.nan
            
            sig['mo']['mttr'] = sig_tasks_by_date[['Zone_Group', 'Corridor', 'Month', 'mttr']].copy()
            if 'delta.mttr' in sig_tasks_by_date.columns:
                sig['mo']['mttr']['delta'] = sig_tasks_by_date['delta.mttr']
            else:
                sig['mo']['mttr']['delta'] = np.nan
        
        logger.info(f"Signal data structure built")
        
        # ======================================================================
        # LOAD HEALTH METRICS
        # ======================================================================
        logger.info("Loading health metrics...")
        
        # In R, Health_Metrics.R is sourced here. Assuming it creates health score data.
        # For now, we'll create a placeholder
        health_metrics = {}
        
        # Try to load if health metrics were computed separately
        health_scores = load_pkl_data("health_scores.pkl")
        if not health_scores.empty:
            health_metrics['scores'] = health_scores
        
        # ======================================================================
        # SAVE PACKAGED DATA
        # ======================================================================
        logger.info("Saving packaged data locally...")
        
        # Create data_output directory if it doesn't exist
        os.makedirs('data_output', exist_ok=True)
        
        # Save as pickle files (Python equivalent of .qs)
        with open('data_output/cor.pkl', 'wb') as f:
            pickle.dump(cor, f, protocol=pickle.HIGHEST_PROTOCOL)
        logger.info("Saved cor.pkl")
        
        with open('data_output/sig.pkl', 'wb') as f:
            pickle.dump(sig, f, protocol=pickle.HIGHEST_PROTOCOL)
        logger.info("Saved sig.pkl")
        
        with open('data_output/sub.pkl', 'wb') as f:
            pickle.dump(sub, f, protocol=pickle.HIGHEST_PROTOCOL)
        logger.info("Saved sub.pkl")
        
        # ======================================================================
        # UPLOAD TO AWS S3
        # ======================================================================
        print(f"{datetime.now()} Upload to AWS [28 of 29 (mark1)]")
        
        try:
            upload_to_s3(cor, sig, sub, conf)
        except Exception as e:
                logger.warning(f"Could not upload to S3: {e}")
                logger.warning(traceback.format_exc())
        
        # ======================================================================
        # WRITE TO DATABASE
        # ======================================================================
        print(f"{datetime.now()} Write to Database [29 of 29 (mark1)]")
        
        try:
            write_to_database(cor, sig, sub, conf)
        except Exception as e:
                logger.warning(f"Could not write to database: {e}")
                logger.warning(traceback.format_exc())
        
        print(f"{datetime.now()} Package for Monthly Report completed successfully")
        logger.info("Monthly Report Package 2 processing completed successfully")
        
    except Exception as e:
        logger.error(f"Error in main processing: {e}")
        logger.error(traceback.format_exc())
        raise
    
    finally:
        # Cleanup
        gc.collect()
        

def upload_to_s3(cor, sig, sub, conf):
    """Upload packaged data to S3 - matches R aws.s3::put_object()"""
    try:
        # Load AWS configuration
        with open("Monthly_Report_AWS.yaml", 'r') as file:
            aws_conf = yaml.safe_load(file)
        
        # Initialize S3 client
        s3_client = boto3.client(
            's3',
            aws_access_key_id=aws_conf['AWS_ACCESS_KEY_ID'],
            aws_secret_access_key=aws_conf['AWS_SECRET_ACCESS_KEY'],
            region_name=aws_conf['AWS_DEFAULT_REGION']
        )
        
        bucket_name = aws_conf.get('bucket', conf.get('bucket', 'gdot-spm'))
        
        # Upload cor, sig, sub as pickle files (equivalent to .qs in R)
        # In R: aws.s3::put_object(file = "cor.qs", object = "cor_ec2.qs", bucket = conf$bucket, multipart = TRUE)
        
        logger.info("Uploading cor to S3...")
        cor_bytes = pickle.dumps(cor, protocol=pickle.HIGHEST_PROTOCOL)
        s3_client.put_object(
            Body=cor_bytes,
            Bucket=bucket_name,
            Key='cor_ec2.pkl'
        )
        logger.info(f"Uploaded cor_ec2.pkl to s3://{bucket_name}/")
        
        logger.info("Uploading sig to S3...")
        sig_bytes = pickle.dumps(sig, protocol=pickle.HIGHEST_PROTOCOL)
        s3_client.put_object(
            Body=sig_bytes,
            Bucket=bucket_name,
            Key='sig_ec2.pkl'
        )
        logger.info(f"Uploaded sig_ec2.pkl to s3://{bucket_name}/")
        
        logger.info("Uploading sub to S3...")
        sub_bytes = pickle.dumps(sub, protocol=pickle.HIGHEST_PROTOCOL)
        s3_client.put_object(
            Body=sub_bytes,
            Bucket=bucket_name,
            Key='sub_ec2.pkl'
        )
        logger.info(f"Uploaded sub_ec2.pkl to s3://{bucket_name}/")
        
        logger.info("All data successfully uploaded to S3")
        
    except Exception as e:
        logger.error(f"Error uploading to S3: {e}")
        raise


def write_to_database(cor, sig, sub, conf):
    """
    Write packaged data to Aurora database - matches R append_to_database()
    
    Writes cor, sub, sig data structures to database tables.
    Each structure becomes multiple tables: {name}_{period}_{metric}
    
    Args:
        cor: Corridor data structure
        sig: Signal data structure  
        sub: Subcorridor data structure
        conf: Configuration dict (not used, kept for compatibility)
    """
    try:
        # Import database writing functions from existing modules
        from database_functions import get_aurora_connection
        from write_sigops_to_db import append_to_database
        from utilities import keep_trying
        
        logger.info("Connecting to Aurora database...")
        
        # Get database connection with retries (matches R code exactly)
        # Note: keep_trying uses 'timeout' parameter, not 'sleep'
        conn = keep_trying(func=get_aurora_connection, n_tries=6, timeout=10)
        logger.info("Database connection created")
        
        try:
            # Get date ranges from configuration or data
            # In R, these come from global variables: calcs_start_date, report_start_date
            # For now, use None (will write all data)
            calcs_start_date = None
            report_start_date = None
            report_end_date = None
            
            # Try to extract dates from the data if available
            if 'mo' in cor and 'vpd' in cor['mo'] and isinstance(cor['mo']['vpd'], pd.DataFrame) and not cor['mo']['vpd'].empty:
                if 'Month' in cor['mo']['vpd'].columns:
                    months = pd.to_datetime(cor['mo']['vpd']['Month'])
                    calcs_start_date = months.min().date()
            
            # Write cor data (matches R: append_to_database(conn, cor, "cor", ...))
            logger.info("Writing corridor (cor) data to database...")
            append_to_database(
                conn, cor, "cor",
                calcs_start_date=calcs_start_date,
                report_start_date=report_start_date,
                report_end_date=report_end_date
            )
            
            # Write sub data
            logger.info("Writing subcorridor (sub) data to database...")
            append_to_database(
                conn, sub, "sub",
                calcs_start_date=calcs_start_date,
                report_start_date=report_start_date,
                report_end_date=report_end_date
            )
            
            # Write sig data
            logger.info("Writing signal (sig) data to database...")
            append_to_database(
                conn, sig, "sig",
                calcs_start_date=calcs_start_date,
                report_start_date=report_start_date,
                report_end_date=report_end_date
            )
            
            logger.info("Successfully wrote all data to database")
            
        finally:
            # Always close connection (matches R: dbDisconnect(conn))
            conn.close()
            logger.info("Database connection closed")
        
    except ImportError as e:
        logger.warning(f"Database writing module not available: {e}")
        logger.info("Skipping database write")
    except Exception as e:
        logger.error(f"Error writing to database: {e}")
        logger.error(traceback.format_exc())
        # Don't raise - allow script to complete even if database write fails


if __name__ == "__main__":
    """Main execution block"""
    
    try:
        print("=" * 80)
        print("Monthly Report Package 2 - Data Packaging for FlexDashboard")
        print("=" * 80)
        
        start_time = datetime.now()
        
        # Run main processing
        main()
        
        # Calculate processing time
        end_time = datetime.now()
        processing_time = end_time - start_time
        
        print(f"\nProcessing completed in {processing_time}")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\nProcessing interrupted by user")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"Fatal error in Monthly Report Package 2: {e}")
        logger.error(traceback.format_exc())
        print(f"\nProcessing failed with error: {e}")
        sys.exit(1)
